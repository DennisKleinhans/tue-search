import csv
import nltk
import ssl
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from typing import List

#Reading the csv file generated by the crawler
def read_csv_to_list(csvfile: str) -> List[List[str]]:
    rows = []
    with open(csvfile, 'r') as file:
        reader = csv.reader(file)
        for row in reader:
            rows.append(row)
    return rows

# Usage
csvfile = 'tmp/documents.csv'
csv_data = read_csv_to_list(csvfile)

#Necessary for downloading the punkt tokenizer and avoid ssl error.
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

#Check if lemmatizaion is necessary with team. 
def lemmatize_tokens(tokens):
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]
    return lemmatized_tokens

def tokenize_docs(data):
    stop_words = set(stopwords.words('english'))  # Example: Remove stop words for English
    tokenized_docs = []
    for doc in data:
        # Assuming text to tokenize is in the 4th column (index 3)
        text = doc[3]
        # Tokenize the text
        tokens = word_tokenize(text)
        # Remove special characters and lowercase
        cleaned_tokens = [token.lower() for token in tokens if re.match(r'^[a-zA-Z0-9äöüß]+$', token)]
        # Remove stopwords and lemmatize
        final_tokens = [token for token in cleaned_tokens if token not in stop_words]
        lemmatized_tokens = lemmatize_tokens(final_tokens)
        tokenized_docs.append(lemmatized_tokens)
    return tokenized_docs

#print(tokenize_docs(csv_data)[1])

#tokenized_docs contains all of the documents text (row document) as word tokens. 
#Also contained are . , (,) etc. at this stage.
tokenized_docs = tokenize_docs(csv_data)

print(tokenized_docs[1])

